{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E1.1: Label Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.model import LabelModel\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from my_utils import threshold_predict, threshold_suppressible_predict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant parts of label matrix\n",
    "lf_subset = list(range(57)) \n",
    "L_data = np.copy(L_alarms[:,lf_subset])\n",
    "Y_data = alarms_df.true_label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"accuracy\",\"coverage\",\"precision\",\"recall\",\"f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cmtx = np.zeros((3,3))\n",
    "\n",
    "thresholds = np.arange(0.5, 1.0, 0.01)\n",
    "m = np.zeros((5,len(thresholds),3))   # for each fold, for each threshold, for each metric\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(kf.split(L_data)):\n",
    "    # Define training dataset\n",
    "    L_train = L_data[train_idx]\n",
    "    Y_train = Y_data[train_idx]\n",
    "    # Define test dataset\n",
    "    L_test = L_data[test_idx]\n",
    "    Y_test = Y_data[test_idx]\n",
    "\n",
    "    # Fit a Label Model\n",
    "    l_model = LabelModel(cardinality=2, verbose=True)\n",
    "    l_model.fit(L_train, n_epochs=100, lr=0.01)\n",
    "    Y_pred, Y_prob = l_model.predict(L_test, return_probs=True, tie_break_policy=\"abstain\")\n",
    "\n",
    "    # Evaluate model performance\n",
    "    all_cmtx = all_cmtx + confusion_matrix(Y_test, Y_pred)\n",
    "    scores = l_model.score(L_test, Y_test, metrics=metrics)\n",
    "    scores[\"abstain\"] = np.sum(Y_pred == ABSTAIN) / len(Y_pred)\n",
    "    scores[\"suppress\"] = np.sum(Y_pred == SUPPRESSIBLE) / len(Y_pred)\n",
    "    logging.info(\"Iteration \" + str(i+1) + \": \", scores)\n",
    "\n",
    "    # Evaluate model with thresholding suppressible predictions\n",
    "    abstain, accuracy, f1 = [], [], []\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        #Y_pred = np.apply_along_axis(threshold_suppressible_predict, 1, Y_prob, thresh)\n",
    "        Y_pred = np.apply_along_axis(threshold_predict, 1, Y_prob, thresh)\n",
    "        Y_pred_noa = Y_pred[Y_pred != ABSTAIN]\n",
    "        Y_test_noa = Y_test[Y_pred != ABSTAIN]\n",
    "        abstain.append( np.sum(Y_pred == ABSTAIN) / len(Y_pred) )\n",
    "        accuracy.append( accuracy_score(Y_test_noa, Y_pred_noa) )\n",
    "        f1.append( f1_score(Y_test_noa, Y_pred_noa) )\n",
    "\n",
    "    m[i,:,0] = abstain\n",
    "    m[i,:,1] = accuracy\n",
    "    m[i,:,2] = f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize metrics\n",
    "print(\"-- Summary (LM, 5-Fold CV) --\")\n",
    "print(all_cmtx)\n",
    "print(\"accuracy: \", (all_cmtx[1,1] + all_cmtx[2,2]) / np.sum(all_cmtx[1:,1:]))\n",
    "print(\"coverage: \", np.sum(all_cmtx[1:,1:]) / np.sum(all_cmtx))\n",
    "print(\"precision: \", all_cmtx[2,2] / np.sum(all_cmtx[1:,2]))\n",
    "print(\"recall: \", all_cmtx[2,2] / np.sum(all_cmtx[2,1:]))\n",
    "print(\"f1: \", all_cmtx[2,2] / (all_cmtx[2,2] + 0.5 * (all_cmtx[1,2] + all_cmtx[2,1])))\n",
    "print(\"abstain: \", np.sum(all_cmtx[:,0]) / np.sum(all_cmtx))\n",
    "print(\"suppress: \", np.sum(all_cmtx[:,2]) / np.sum(all_cmtx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe effect of thresholding suppressible predictions\n",
    "x = np.arange(0.5, 1.0, 0.05)\n",
    "avg_thresh_scores = np.mean(m, axis=0)\n",
    "plt.plot(avg_thresh_scores[:,0], avg_thresh_scores[:,1])\n",
    "plt.plot(avg_thresh_scores[:,0], avg_thresh_scores[:,2])\n",
    "plt.xlabel(\"Abstain\")\n",
    "plt.ylabel(\"%\")\n",
    "plt.legend((\"Accuracy\",\"F1\"), bbox_to_anchor=(1,1))\n",
    "plt.title(\"Effect of thresholding predictions\")\n",
    "#plt.title(\"Effect of thresholding suppressible predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_thresh_scores)"
   ]
  }
 ]
}