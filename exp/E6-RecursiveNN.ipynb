{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597768490667",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E6: Recursive NN\n",
    "\n",
    "Experimental Algorithm:\n",
    "- 57 LFs -> 20 randomly sampled sets of 5 -> Snorkel -> 20 LFs -> analysis\n",
    "- 20 LFs -> 20 randomly sampled sets of 5 -> Snorkel -> 20 LFs -> analysis\n",
    "- repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.model.label_model import LabelModel\n",
    "from snorkel.analysis import metric_score\n",
    "from Our_Monitors.CD_Monitor import Informed_LabelModel\n",
    "#from Our_Monitors.CDGA_Monitor import CDGAM\n",
    "from Our_Monitors.CDGA_Monitor_fast import CDGAM_fast as CDGAM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from my_utils import predict_at_abstain_rate\n",
    "import numpy as np\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_iters = N      # N\n",
    "num_subsets = J    # J\n",
    "subset_size = K    # K\n",
    "with_replacement = True\n",
    "\n",
    "#abstain_target = A\n",
    "\n",
    "sig = 0.05\n",
    "policy = \"new\"\n",
    "\n",
    "metrics = [\"accuracy\",\"f1\",\"coverage\"]\n",
    "\n",
    "lf_subset = list(range(57))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a subset of the LFs in the analysis\n",
    "L_data = np.copy(L_alarms[:,lf_subset])\n",
    "Y_data = alarms_df.true_label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed as current time to get different results\n",
    "np.random.seed(int(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = np.zeros((5,num_iters))\n",
    "\n",
    "for iter in range(num_iters):\n",
    "    logging.info(\"-- Iteration \" + str(iter + 1) + \"--\")\n",
    "\n",
    "    # Randomly sample subsets of specified size\n",
    "    try:\n",
    "        subsets = np.random.choice(L_data.shape[1], size=(num_subsets,subset_size), replace=with_replacement)\n",
    "    except ValueError:\n",
    "        logging.error(\"Cannot perform sample...\")\n",
    "        break\n",
    "\n",
    "    # Define a new LF per subset as a Snorkel model over the LFs in the subset\n",
    "    new_L_data = np.zeros((L_data.shape[0], num_subsets))\n",
    "\n",
    "    for i, subset in enumerate(subsets):\n",
    "        l_model = LabelModel(cardinality=2, verbose=True)\n",
    "        l_model.fit(L_data[:,subset], n_epochs=100, lr=0.01)\n",
    "        new_L_data[:,i] = l_model.predict(L_data[:,subset])\n",
    "\n",
    "    L_data = np.copy(new_L_data)\n",
    "\n",
    "    # Learn dependencies between new LFs (CDGAM)\n",
    "    L_train, L_dev, Y_train, Y_dev = train_test_split(L_data, Y_data, test_size=0.2, shuffle=True)\n",
    "    deps = CDGAM(L_dev, k=2, sig=sig, policy=policy, verbose=False, return_more_info=False)\n",
    "\n",
    "    # Evaluate Informed Label Model on new LFs\n",
    "    il_model = Informed_LabelModel(deps, cardinality=2, verbose=True)\n",
    "    il_model.fit(L_train, n_epochs=100, lr=0.01)\n",
    "    scores = il_model.score(L_train, Y_train, metrics=metrics)\n",
    "    \n",
    "    #Y_prob = il_model.predict_proba(L_train)\n",
    "    #Y_pred = predict_at_abstain_rate(Y_prob, abstain_target)\n",
    "    #scores = {}\n",
    "    #scores[\"accuracy\"] = metric_score(Y_train, Y_pred, metric=\"accuracy\")\n",
    "    #Y_train_temp = Y_train[Y_pred != ABSTAIN]\n",
    "    #Y_pred_temp = Y_pred[Y_pred != ABSTAIN]\n",
    "    #scores[\"f1\"] = metric_score(Y_train_temp, Y_pred_temp, metric=\"f1\")\n",
    "    #scores[\"abstain\"] = 1.0 - metric_score(Y_train, Y_pred, metric=\"coverage\")\n",
    "    \n",
    "    scores[\"abstain\"] = 1 - scores[\"coverage\"]\n",
    "    scores[\"num deps (CDGAM)\"] = len(deps)\n",
    "\n",
    "    # Logging\n",
    "    logging.info(scores)\n",
    "    all_scores[:,iter] = list(scores.values())"
   ]
  }
 ]
}