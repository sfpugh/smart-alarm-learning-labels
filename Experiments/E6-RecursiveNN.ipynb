{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E6: Recursive NN\n",
    "\n",
    "Experimental Algorithm:\n",
    "- 57 LFs -> 20 randomly sampled sets of 5 -> Snorkel -> 20 LFs -> analysis\n",
    "- 20 LFs -> 20 randomly sampled sets of 5 -> Snorkel -> 20 LFs -> analysis\n",
    "- repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.model.label_model import LabelModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Our_Monitors.CD_Monitor import CDM\n",
    "from Our_Monitors.CDGA_Monitor import CDGAM\n",
    "import numpy as np\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the analysis\n",
    "num_iters = 10\n",
    "num_subsets = 20\n",
    "subset_size = 5\n",
    "with_replacement = True\n",
    "lf_subset = list(range(57))\n",
    "metrics = [\"accuracy\",\"f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a subset of the LFs in the analysis\n",
    "L_data = np.copy(L_alarms[:,lf_subset])\n",
    "Y_data = alarms_df.true_label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed as current time to get different results\n",
    "np.random.seed(int(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = []\n",
    "all_deps = []\n",
    "\n",
    "for iter in range(num_iters):\n",
    "    logging.info(\"-- Iteration \", iter + 1, \"--\")\n",
    "\n",
    "    # Randomly sample subsets of specified size\n",
    "    try:\n",
    "        subsets = np.random.choice(L_data.shape[1], size=(num_subsets,subset_size), replace=with_replacement)\n",
    "    except ValueError:\n",
    "        print(\"Cannot perform sample...\")\n",
    "        break\n",
    "\n",
    "    # Define a new LF per subset as a Snorkel model over the LFs in the subset\n",
    "    new_L_data = np.zeros((L_data.shape[0], num_subsets))\n",
    "    \n",
    "    for i, subset in enumerate(subsets):\n",
    "        l_model = LabelModel(cardinality=2, verbose=True)\n",
    "        l_model.fit(L_data[:,subset])\n",
    "        new_L_data[:,i] = l_model.predict(L_data[:,subset])\n",
    "\n",
    "    L_data = np.copy(new_L_data)\n",
    "\n",
    "    # Evaluate model trained over new LFs\n",
    "    l_model = LabelModel(cardinality=2, verbose=True)\n",
    "    l_model.fit(L_data)\n",
    "    scores = l_model.score(L_data, Y_data, metrics=metrics)\n",
    "    logging.info(scores)\n",
    "    all_scores.append(scores)\n",
    "\n",
    "    # Find dependencies between new LFs\n",
    "    L_train, L_dev, Y_train, Y_dev = train_test_split(L_data, Y_data, test_size=0.2, shuffle=True)\n",
    "    #deps = CDM(L_dev, Y_dev, k=2, sig=0.05, policy=\"old\", verbose=False)\n",
    "    deps = CDGAM(L_dev, k=2, sig=0.05, policy=\"new\", verbose=False, return_more_info=False)\n",
    "    print(len(deps))\n",
    "    all_deps.append(len(deps))"
   ]
  }
 ]
}