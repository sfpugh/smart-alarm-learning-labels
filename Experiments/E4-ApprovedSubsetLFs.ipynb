{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "smart_alarm_env",
   "display_name": "smart_alarm_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E4: Using a Clinician-Approved Subset of Labeling Functions\n",
    "It has been observed that our set of 57 labeling functions are likely redundant and overly dependent, to the extent that model performance is suffering. So we asked Chris to give us a subset of 10-15 of our labeling functions that he finds \"most important\" for our task. The following code implements a Label Model using this subset and evaluates the new model.\n",
    "\n",
    "The list of LFs in the subset are on the \"Implements LFs\" tab at the following link: https://docs.google.com/spreadsheets/d/1_1QBVaiWl4SkBy9vEFBk5Uv0HfPWUMX8Sg_IPZoJbfA/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from snorkel.labeling.model.label_model import LabelModel\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant parts of label matrix\n",
    "\n",
    "lf_subset = [57, 58, 59, 60, 61, 17, 23, 30, 41, 52]\n",
    "L_data = np.copy(L_alarms[:,lf_subset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uninformed Label Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cmtx = np.zeros((3,3))\n",
    "metrics = [\"accuracy\",\"coverage\",\"precision\",\"recall\",\"f1\",\"roc_auc\"]\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(kf.split(L_data)):\n",
    "    # Define training dataset\n",
    "    L_train = L_data[train_idx]\n",
    "    Y_train = alarms_df.true_label.values[train_idx]\n",
    "    # Define test dataset\n",
    "    L_test = L_data[test_idx]\n",
    "    Y_test = alarms_df.true_label.values[test_idx]\n",
    "\n",
    "    # Fit a label model\n",
    "    l_model = LabelModel(cardinality=2, verbose=True)\n",
    "    l_model.fit(L_train, n_epochs=100, log_freq=10, seed=SEED)\n",
    "\n",
    "    # Evaluate\n",
    "    scores = l_model.score(L_test, Y=Y_test, metrics=metrics, tie_break_policy=\"abstain\")\n",
    "    Y_pred = l_model.predict(L_test, tie_break_policy=\"abstain\")\n",
    "    conf_mtx = confusion_matrix(Y_test, Y_pred)\n",
    "    all_cmtx = all_cmtx + conf_mtx\n",
    "\n",
    "    logging.info(\"-- ITERATION \", i+1, \" --\")\n",
    "    logging.info(conf_mtx)\n",
    "    logging.info(\"Abstain: \", np.sum(Y_pred == ABSTAIN) / len(Y_pred))\n",
    "    logging.info(\"Supress: \", np.sum(Y_pred == SUPPRESSIBLE) / len(Y_pred))\n",
    "    logging.info(\"Scores: \", scores, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarized scores from summing confusion matrices over iterations\n",
    "\n",
    "print(\"-- SUMMARY (Uninformed) --\")\n",
    "print(all_cmtx)\n",
    "print(\"Abstain: \", np.sum(Y_pred == ABSTAIN) / len(Y_pred))\n",
    "print(\"Supress: \", np.sum(Y_pred == SUPPRESSIBLE) / len(Y_pred))\n",
    "print(\"accuracy: \", (all_cmtx[1,1] + all_cmtx[2,2]) / np.sum(all_cmtx[1:,1:]))\n",
    "print(\"coverage: \", np.sum(all_cmtx[1:,1:]) / np.sum(all_cmtx))\n",
    "print(\"precision: \", all_cmtx[2,2] / np.sum(all_cmtx[1:,2]))\n",
    "print(\"recall: \", all_cmtx[2,2] / np.sum(all_cmtx[2,1:]))\n",
    "print(\"f1: \", all_cmtx[2,2] / (all_cmtx[2,2] + 0.5 *(all_cmtx[1,2] + all_cmtx[2,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informed Label Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Our_Monitors.CD_Monitor import CDM, Informed_LabelModel\n",
    "from Our_Monitors.CDGA_Monitor import CDGAM\n",
    "from Our_Monitors.New_Monitor import NM\n",
    "\n",
    "all_cmtx = np.zeros((3,3))\n",
    "metrics = [\"accuracy\",\"coverage\",\"precision\",\"recall\",\"f1\",\"roc_auc\"]\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "n_deps = []\n",
    "\n",
    "for i, (train_dev_idx, test_idx) in enumerate(kf.split(L_data)):\n",
    "    train_idx, dev_idx = train_test_split(train_dev_idx, test_size=0.25, random_state=SEED)\n",
    "\n",
    "    # Define training dataset\n",
    "    L_train = L_data[train_idx]\n",
    "    Y_train = alarms_df.true_label.values[train_idx]\n",
    "    # Define development dataset\n",
    "    L_dev = L_data[dev_idx]\n",
    "    Y_dev = alarms_df.true_label.values[dev_idx]\n",
    "    # Define test dataset\n",
    "    L_test = L_data[test_idx]\n",
    "    Y_test = alarms_df.true_label.values[test_idx]\n",
    "\n",
    "    # Get edges of dependency graph from Conditional Dependency Monitor (CDM)\n",
    "    deps = CDM(L_dev, Y_dev, k=2, sig=0.05, policy=\"old\", verbose=False)\n",
    "    #deps = CDGAM(L_dev, k=2, sig=0.05, policy=\"new\", verbose=False, return_more_info=False)\n",
    "    #deps = NM(L_dev, Y_dev, k=2, sig=0.05, policy=\"new\", verbose=False, return_more_info=False)\n",
    "    n_deps.append(len(deps))\n",
    "\n",
    "    # Fit an informed label model\n",
    "    l_model = Informed_LabelModel(edges=deps, cardinality=2, verbose=True)\n",
    "    l_model.fit(L_train, n_epochs=100, log_freq=10, seed=SEED)\n",
    "\n",
    "    # Evaluate\n",
    "    scores = l_model.score(L_test, Y=Y_test, metrics=metrics, tie_break_policy=\"abstain\")\n",
    "    Y_pred = l_model.predict(L_test, tie_break_policy=\"abstain\")\n",
    "    conf_mtx = confusion_matrix(Y_test, Y_pred)\n",
    "    all_cmtx = all_cmtx + conf_mtx\n",
    "\n",
    "    logging.info(\"-- ITERATION \", i+1, \" --\")\n",
    "    logging.info(conf_mtx)\n",
    "    logging.info(\"Abstain: \", np.sum(Y_pred == ABSTAIN) / len(Y_pred))\n",
    "    logging.info(\"Supress: \", np.sum(Y_pred == SUPPRESSIBLE) / len(Y_pred))\n",
    "    logging.info(\"Scores: \", scores, '\\n')\n",
    "    logging.info(\"Num deps: \", len(deps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarized scores from summing confusion matrices over iterations\n",
    "\n",
    "print(\"-- SUMMARY (Informed) --\")\n",
    "print(all_cmtx)\n",
    "print(\"Abstain: \", np.sum(Y_pred == ABSTAIN) / len(Y_pred))\n",
    "print(\"Supress: \", np.sum(Y_pred == SUPPRESSIBLE) / len(Y_pred))\n",
    "print(\"accuracy: \", (all_cmtx[1,1] + all_cmtx[2,2]) / np.sum(all_cmtx[1:,1:]))\n",
    "print(\"coverage: \", np.sum(all_cmtx[1:,1:]) / np.sum(all_cmtx))\n",
    "print(\"precision: \", all_cmtx[2,2] / np.sum(all_cmtx[1:,2]))\n",
    "print(\"recall: \", all_cmtx[2,2] / np.sum(all_cmtx[2,1:]))\n",
    "print(\"f1: \", all_cmtx[2,2] / (all_cmtx[2,2] + 0.5 *(all_cmtx[1,2] + all_cmtx[2,1])))\n",
    "print(\"Average num deps: \", np.mean(n_deps))"
   ]
  }
 ]
}