{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "smart_alarm_env",
   "display_name": "smart_alarm_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E4: Using a Clinician-Approved Subset of Labeling Functions\n",
    "It has been observed that our set of 57 labeling functions are likely redundant and overly dependent, to the extent that model performance is suffering. So we asked Chris to give us a subset of 10-15 of our labeling functions that he finds \"most important\" for our task. The following code implements a Label Model using this subset and evaluates the new model.\n",
    "\n",
    "The list of LFs in the subset are on the \"Implements LFs\" tab at the following link: https://docs.google.com/spreadsheets/d/1_1QBVaiWl4SkBy9vEFBk5Uv0HfPWUMX8Sg_IPZoJbfA/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant parts of label matrix\n",
    "\n",
    "lf_subset = [57, 58, 59, 60, 61, 17, 23, 30, 41, 52]\n",
    "\n",
    "L_data = np.copy(L_alarms[:,lf_subset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from snorkel.labeling.model.label_model import LabelModel\n",
    "\n",
    "all_scores = []\n",
    "all_cmtx = np.zeros((3,3))\n",
    "metrics = [\"accuracy\",\"coverage\",\"precision\",\"recall\",\"f1\",\"roc_auc\"]\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(kf.split(L_data)):\n",
    "    # Define training dataset\n",
    "    L_train = L_data[train_idx]\n",
    "    Y_train = alarms_df.true_label.values[train_idx]\n",
    "    # Define test dataset\n",
    "    L_test = L_data[test_idx]\n",
    "    Y_test = alarms_df.true_label.values[test_idx]\n",
    "\n",
    "    # Fit a label model\n",
    "    l_model = LabelModel(cardinality=2, verbose=True)\n",
    "    l_model.fit(L_train, n_epochs=100, log_freq=10, seed=SEED)\n",
    "\n",
    "    # Evaluate\n",
    "    scores = l_model.score(L_test, Y=Y_test, metrics=metrics, tie_break_policy=\"abstain\")\n",
    "    Y_pred = l_model.predict(L_test, tie_break_policy=\"abstain\")\n",
    "    conf_mtx = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "    all_cmtx = all_cmtx + conf_mtx\n",
    "    all_scores.append(scores)\n",
    "\n",
    "    print(\"-- ITERATION \", i+1, \" --\")\n",
    "    print(conf_mtx)\n",
    "    print(\"Abstain: \", np.sum(Y_pred == ABSTAIN) / len(Y_pred))\n",
    "    print(\"Supress: \", np.sum(Y_pred == SUPPRESSIBLE) / len(Y_pred))\n",
    "    print(\"Scores: \", scores, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarized scores from summing confusion matrices over iterations\n",
    "\n",
    "print(\"-- SUMMARY --\")\n",
    "print(all_cmtx)\n",
    "print(\"Abstain: \", np.sum(Y_pred == ABSTAIN) / len(Y_pred))\n",
    "print(\"Supress: \", np.sum(Y_pred == SUPPRESSIBLE) / len(Y_pred))\n",
    "print(\"accuracy: \", (all_cmtx[1,1] + all_cmtx[2,2]) / np.sum(all_cmtx[1:,1:]))\n",
    "print(\"coverage: \", np.sum(all_cmtx[1:,1:]) / np.sum(all_cmtx))\n",
    "print(\"precision: \", all_cmtx[2,2] / np.sum(all_cmtx[1:,2]))\n",
    "print(\"recall: \", all_cmtx[2,2] / np.sum(all_cmtx[2,1:]))\n",
    "print(\"f1: \", all_cmtx[2,2] / (all_cmtx[2,2] + 0.5 *(all_cmtx[1,2] + all_cmtx[2,1])))"
   ]
  }
 ]
}